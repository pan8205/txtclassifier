{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 760 (CNMeM is disabled, cuDNN not available)\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "\n",
    "#from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers.core import Flatten #, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#from multiprocessing import cpu_count\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Numpy random seed\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "# data and model folder\n",
    "data_folder = '.\\\\data\\\\'\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prepare content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0] topic, place, people, organization, exchanges\n",
      "sandoz ag said it planned a joint venture\n",
      "to produce herbicides in the soviet union.\n",
      "    the company said it had signed a letter of intent with the\n",
      "soviet ministry of fertiliser production to form the first\n",
      "foreign joint venture the ministry had undertaken since the\n",
      "soviet union allowed western firms to enter into joint ventures\n",
      "two months ago.\n",
      "    the ministry and sandoz will each have a 50 pct stake, but\n",
      "a company spokeswoman was unable to give details of the size of\n",
      "investment or planned output.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(data_folder + 'content.json', 'r') as f:\n",
    "     content = json.load(f)\n",
    "\n",
    "with open(data_folder + 'label.json', 'r') as f:\n",
    "     label = json.load(f)\n",
    "\n",
    "print label['1001'], 'topic, place, people, organization, exchanges'\n",
    "print content['1001']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenize contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenized content collection\n",
    "content_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Anaconda2\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "17743\n",
      "[u'sandoz', u'ag', u'said', u'planned', u'joint', u'venture', u'produce', u'herbicide', u'soviet', u'union', u'company', u'said', u'signed', u'letter', u'intent', u'soviet', u'ministry', u'fertiliser', u'production', u'form', u'first', u'foreign', u'joint', u'venture', u'ministry', u'undertaken', u'since', u'soviet', u'union', u'allowed', u'western', u'firm', u'enter', u'joint', u'venture', u'two', u'month', u'ago', u'ministry', u'sandoz', u'pct', u'stake', u'company', u'spokeswoman', u'unable', u'give', u'detail', u'size', u'investment', u'planned', u'output']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n",
    "\n",
    "content_documents = []\n",
    "# Tokenize\n",
    "key1001 = 0\n",
    "for key in content.keys():\n",
    "    if key == '1001':\n",
    "        key1001 = len(content_documents)\n",
    "        print len(content_documents)\n",
    "        print tokenize(content[key])\n",
    "    content_documents.append(tokenize(content[key]))\n",
    "\n",
    "number_of_documents = len(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Gensim Word2Vec model\n",
    "w2v_model = Word2Vec(content_documents, size=num_features, min_count=1, window=10)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vectorize content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02157213 -0.04568234  0.00146626 ..., -0.00972886 -0.01741979\n",
      "  -0.00713177]\n",
      " [-0.03147293 -0.0149641   0.03508525 ..., -0.03686867 -0.00689081\n",
      "  -0.02585261]\n",
      " [-0.02510744 -0.00800088 -0.00946434 ...,  0.08622478  0.01505131\n",
      "  -0.05790702]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "num_categories = 5\n",
    "X = numpy.zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(numpy.float32)\n",
    "Y = numpy.zeros(shape=(number_of_documents, num_categories)).astype(numpy.float32)\n",
    "\n",
    "empty_word = numpy.zeros(num_features).astype(numpy.float32)\n",
    "\n",
    "for idx, document in enumerate(content_documents):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                X[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                X[idx, jdx, :] = empty_word\n",
    "\n",
    "for idx, key in enumerate(label.keys()):\n",
    "    Y[idx, :] = label[key]\n",
    "    \n",
    "print X[key1001]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
